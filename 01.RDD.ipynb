{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD represents Resilient Distributed Dataset. An RDD in Spark is simply an immutable distributed collection of objects sets. Each RDD is split into multiple partitions (similar pattern with smaller sets), which may be computed on different nodes of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By using `parallelize()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark create RDD example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df = spark.sparkContext.parallelize([(1,2,3,'a b c'),\n",
    "                                    (4,5,6,'d e f'),\n",
    "                                    (7,8,9, 'g,h,i')]).toDF(['col1','col2','col3','col4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Show the RDD data:\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark create RDD example\") \\\n",
    "        .config('spark.some.config.option', 'some-value') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "myData = spark.sparkContext.parallelize([(1,2),(3,4),(5,6),(7,8),(7,9),(9,10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the RDD data:\n",
    "\n",
    "myData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By using `createDataFrame( )` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark create RDD example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "Employee = spark.createDataFrame([\n",
    "                        ('1', 'Joe',   '70000', '1'),\n",
    "                        ('2', 'Henry', '80000', '2'),\n",
    "                        ('3', 'Sam',   '60000', '2'),\n",
    "                        ('4', 'Max',   '90000', '1')],\n",
    "                        ['Id', 'Name', 'Sallary','DepartmentId']\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RDD data can be obtained on two ways:\n",
    "\n",
    "#First option:\n",
    "\n",
    "Employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second option:\n",
    "\n",
    "Employee.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset from csv file\n",
    "\n",
    "#### Using `read()` and `load()` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up  SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark create RDD example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.format('csv').\\\n",
    "                               options(header='true', \\\n",
    "                               inferschema='true').\\\n",
    "                load(\"data/data_by_year.csv\",header=True)\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, RDDs offer two types of operations: \n",
    "\n",
    "-Transformations\n",
    "\n",
    "-Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset from DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up  SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"Python Spark create RDD example\") \\\n",
    "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading tables from Database needs the proper drive for the corresponding Database. For example, the above demo needs org.postgresql.Driver and you need to download it and put it in jars folder of your spark installation path. I download postgresql-42.2.18.jar from the official website and put it in jars folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I NEED TO DEFINED ACCESS TO SQL DB\n",
    "\n",
    "# # ## User information\n",
    "# user = 'your_username'\n",
    "# pw   = 'your_password'\n",
    "\n",
    "# ## Database information\n",
    "# table_name = 'table_name'\n",
    "# url = 'jdbc:postgresql://##.###.###.##:5432/dataset?user='+user+'&password='+pw\n",
    "# properties ={'driver': 'org.postgresql.Driver', 'password': pw,'user': user}\n",
    "\n",
    "# df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
    "\n",
    "# df.show(5)\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
