{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing ApacheSpark on Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A well documented repository for learning ApacheSpark and documentation for use and installation on different opperative systems can be found on:\n",
    "\n",
    "    \n",
    "[LearningApacheSpark](https://runawayhorse001.github.io/LearningApacheSpark/index.html)\n",
    "\n",
    "ApacheSpark can be also run on virtual machines or notebooks provided by cloud services. (We will explore this on the future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebooks summarizes the proccess to install ApacheSpark on a Python environment running on Ubuntu 20.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new environment\n",
    "\n",
    "Just a good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update the package index**\n",
    "\n",
    "`sudo apt update`\n",
    "\n",
    "**Verify if Java is alreay installed**\n",
    "`java -version`\n",
    "\n",
    "If Java is not installed, you can proceed with the installation.\n",
    "\n",
    "**Install the default Java Runtime Environment (JRE)**\n",
    "\n",
    "`sudo apt install default-jre`\n",
    "\n",
    "**Verify the installation **\n",
    "\n",
    "`java -version`\n",
    "\n",
    "**Expected Output:**\n",
    "\n",
    "`openjdk version \"11.0.7\" 2020-04-14\n",
    "OpenJDK Runtime Environment (build 11.0.7+10-post-Ubuntu-3ubuntu1)\n",
    "OpenJDK 64-Bit Server VM (build 11.0.7+10-post-Ubuntu-3ubuntu1, mixed mode, sharing)`\n",
    "\n",
    "**Install the Java Development Kit (JDK) in addition to the JRE **\n",
    "\n",
    "`sudo apt install default-jdk`\n",
    "\n",
    "**Verify that the JDK is installed by checking the version of javac, the Java compiler:**\n",
    "\n",
    "`javac -version`\n",
    "\n",
    "**Expected output:**\n",
    "\n",
    "`javac 11.0.7`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More info for installaing Java \n",
    "\n",
    "Credits to @digitalocean/community\n",
    "\n",
    "[Instructions for Ubuntu 20](https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-on-ubuntu-20-04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ApacheSpark can be installed on your Python environment using *pip*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pip install pyspark```\n",
    "\n",
    "[Documentation can be found here](https://pypi.org/project/pyspark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output\n",
      "\n",
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|   1|   2|   3|a b c|\n",
      "|   4|   5|   6|d e f|\n",
      "|   7|   8|   9|g h i|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Code for testing installation\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark create RDD example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.sparkContext\\\n",
    "          .parallelize([(1, 2, 3, 'a b c'),\n",
    "                        (4, 5, 6, 'd e f'),\n",
    "                        (7, 8, 9, 'g h i')])\\\n",
    "          .toDF(['col1', 'col2', 'col3','col4'])\n",
    "\n",
    "print('Output\\n')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "`+----+----+----+-----+\n",
    "|col1|col2|col3| col4|\n",
    "+----+----+----+-----+\n",
    "|   1|   2|   3|a b c|\n",
    "|   4|   5|   6|d e f|\n",
    "|   7|   8|   9|g h i|\n",
    "+----+----+----+-----+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the output match with the above content, you have installed succesfully ApacheSpark on your Python environment.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
